I was thrilled when I was offered the summer internship for two specific reasons. First, it was with The New York Times and second, I was going to join the Search team. I was looking for an opportunity that intersected with my interests in information retrieval and machine learning. More importantly, I would have a chance to use what I had been learning in grad school. Before starting, I had several assumptions about the internship: I expected that it would be a great learning experience. I would also learn how search systems are structured in a professional setup and what exactly the Search team does everyday. I learned the high level architecture during the first week, and it was way more complicated than I expected.Over time, I learned about several of its components in more depth. By attending daily stand-ups where each team member reviewed what they were going to do that day, I got a detailed idea of their work in the team. Another assumption was that I would be a part of a project going on in the Search team and my work would be limited to a small component of it, but I was wrong. It was made clear that I was going to have my own project and I was given full freedom to decide my own project! The team gave me some awesome ideas, and after brainstorming for about a week I decided on my project: to increase the relevancy of article search results from The New York Times search engine.Sometimes it is difficult to construct a query for the information you are looking for for a multitude of reasons. For instance, if you’re looking for context for a specific term, or missing a particular word entirely, using the search engine can be a hassle. What you have in mind is a vague idea and some generic words, but you are looking for specific information. Since the basic text search algorithms use the query you’ve typed — verbatim — to fetch and rank documents, the results sometimes are not satisfactory, or may be entirely irrelevant to what you are looking for.For example — you know that something happened in the Britain, but you don’t know where and what it is about. If you search for “UK”, the ranking of the document will be based on the traditional tf-idf scoring, so the ranking of a document highly relies on the user query. So instead of documents on Brexit or any other recent event, you will get the documents in which “UK” occurs more frequently.One of the aims of my project was to solve this problem. And thus I designed Word2vec-based query expansion. The high level idea is to take the user query, statistically figure out the most relevant words based on recent events, and add them to the user query to create an expanded query.If the expanded query — with recent, related terms included — fetches and ranks articles, the results should be considerably more relevant to the original user query and recent events. Continuing the previous example, if I can add “Brexit” or “EU” to your query, the chances are higher that you will get more recent information on current events going on in Britain.The most interesting part of this project was how to figure out what words are relevant to a given input word. To accomplish this task, we used Word2vec, a machine learning tool that takes text as input and returns a vector model representing words grouped by their latent contextual meanings.The high level idea of Word2vec is that it’s a two-layer neural net and uses the context of each word to calculate its vector. That means if two words are used in a similar context, the similarity between them would be higher. This vector representation of words allows us to perform all kind of useful math on these vectors. To find similar words to a query, we map the query in the vector space, calculate cosine similarity of each word in the space with the query, and take the most similar words and use them to expand the user query. To test the hypothesis, I trained several Word2vec models with The New York Times text corpus with different parameters. The results were really fascinating and useful. For the word “UK”, following are the closest words with their cosine similarity score.Then I designed an experiment to test if Word2vec-based query expansion actually works. It turned out query expansion outperformed the normal model, and the results were way better than expected with an 18 percent rise in precision, recall and NDCG were also higher.Apart from having my own research project, I also enjoyed my internship because I got to learn Clojure, a LISP that follows a totally different programming paradigm than what I’m used to. In addition, the guidance and support I got from the team was tremendous. Even though I was working alone on my project, it never felt like that because of the team. They helped me debug and optimize my code, and even suggested proper directions whenever I was stuck somewhere. I learned a lot more than coding this summer.Tushar Baraiya is a summer intern on the Search team. He will return to University at Buffalo in the fall as a graduate student in computer science.